{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9_y5BmOggBrY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\sarah\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\sarah\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\sarah\\anaconda3\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sarah\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "C:\\Users\\sarah\\OneDrive\\Desktop\\rob599-3D_perception\\point_clouds\n",
      "['.ipynb_checkpoints', 'data', 'Homework2.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# print(sys.executable)\n",
    "!pip install torch torchvision torchaudio\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from google.colab import drive\n",
    "from tqdm import tqdm\n",
    "\n",
    "running_in_colab = 'google.colab' in str(get_ipython())\n",
    "\n",
    "if running_in_colab == True:\n",
    "    drive.mount('/content/gdrive/')\n",
    "    GOOGLE_DRIVE_PATH_AFTER_GDRIVE = '#####YOUR PATH HERE####### '\n",
    "    GOOGLE_DRIVE_PATH = os.path.join('gdrive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_GDRIVE)\n",
    "    os.chdir(os.path.join('/content',GOOGLE_DRIVE_PATH))\n",
    "else:\n",
    "    GOOGLE_DRIVE_PATH = os.path.join('C:\\\\Users\\\\sarah\\\\OneDrive\\\\Desktop\\\\rob599-3D_perception\\\\point_clouds')\n",
    "    os.chdir(os.path.join('/content',GOOGLE_DRIVE_PATH))\n",
    "\n",
    "print(GOOGLE_DRIVE_PATH)\n",
    "print(os.listdir(GOOGLE_DRIVE_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l_jK986WKQ81"
   },
   "source": [
    "# Instructions:\n",
    "Download the data from this [link](https://colab.research.google.com/drive/16Q3M_H6L1l_y2xu2MQP6icj6iN75zu18?usp=sharing). Unzip and upload it to your google drive in a folder called ```Homework2```. Make sure all files are stored in a folder called ```data``` inside the folder ```Homework2```. Copy the path after ```gdrive/``` to folder ```Homework2``` and paste it in the space above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-V6b30NRiayH"
   },
   "source": [
    "## Q1a. Getting a point cloud from a Depth image\n",
    "\n",
    "You are given a rgb image, a depth image and a mask. Your task is to generate point cloud data from the given depth image and the corresponding colour from the RGB image. Recall we used projection equations to project 3D points to a 2D plane. Here, given depth, we need to reproject the points back onto a 3D plane. You are also given the camera data (focal length and camera center). Use the data from file ```generate_point_cloud.npz```\n",
    "\n",
    "```CORRECTION: The pricipal points are not given in the file. Here are the values for the principal point: 0.4224, -0.0300```\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1Dng54OFGump4H5-m7BLpgoL3ikMLa7aP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "j4X2QsWrlEvU"
   },
   "outputs": [],
   "source": [
    "def generate_point_clouds(rgb_image, depth_image, mask, camera_data):\n",
    "    \"\"\"\n",
    "    Generate a 3D point cloud from a given RGB image, depth image, and mask using camera data.\n",
    "\n",
    "    Args:\n",
    "        rgb_image (numpy.ndarray): RGB image as a NumPy array.\n",
    "        depth_image (numpy.ndarray): Depth image as a NumPy array.\n",
    "        mask (numpy.ndarray): Binary mask indicating valid pixels in the images.\n",
    "        camera_data (dict): Camera parameters including focal length and camera center.\n",
    "\n",
    "    Returns:\n",
    "        point_cloud (numpy.ndarray): 3D point cloud represented as a NumPy array of shape (N, 3),\n",
    "            where N is the number of valid points.\n",
    "        rgb (numpy.ndarray): RGB color values corresponding to the 3D points, represented as a\n",
    "            NumPy array of shape (N, 3), where N is the number of valid points.\n",
    "    \"\"\"\n",
    "    focal_length = camera_data[\"focal_length\"]\n",
    "    print('focal', np.shape(focal_length))\n",
    "    print(focal_length[0, 0])\n",
    "    camera_center = camera_data[\"camera_center\"]\n",
    "    print('size rgb', np.shape(rgb_image))\n",
    "    print('size depth', np.shape(depth_image))\n",
    "    print('size mask', np.shape(mask))\n",
    "\n",
    "    height, width = np.shape(depth_image)\n",
    "    num_pixels = height * width\n",
    "    N = num_pixels - np.sum(mask == 0)\n",
    "    point_cloud = np.zeros((N, 3))\n",
    "    rgb = np.zeros((N, 3))\n",
    "\n",
    "    rgb_image = rgb_image.reshape((num_pixels, 3))\n",
    "    depth_image = depth_image.reshape((num_pixels))\n",
    "    mask = mask.reshape((num_pixels))\n",
    "    print('size rgb', np.shape(rgb_image))\n",
    "    print('size depth', np.shape(depth_image))\n",
    "    print('size mask', np.shape(mask))\n",
    "\n",
    "    # projection matrix\n",
    "    M = np.array([[focal_length[0, 0], 0, 0.4224], [0, focal_length[0, 1], -0.03], [0, 0, 1]]) @ \\\n",
    "        np.array([[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0]])\n",
    "    M_pinv = M.T @ np.inv(M @ M.T)\n",
    "\n",
    "    index = 0\n",
    "    for h in height:\n",
    "        for w in width:\n",
    "            if mask(i) != 0:\n",
    "                coord = M_pinv @ np.array([[w], [h], [1]])\n",
    "                point_cloud[index] = coord\n",
    "                rgb[index] = rgb[i]\n",
    "                index += 1\n",
    "    \n",
    "    # point_cloud = None\n",
    "    # rgb = None\n",
    "    return point_cloud, rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U7K9dYYLmpPq"
   },
   "source": [
    "## Q1b. Generate Visualization\n",
    "\n",
    "Use the point clouds and rgb data obtained from Q1a and plot them to generate at least 3 different views of the object\n",
    "\n",
    "```Note: You will need to visualize a lot of point clouds in this assignment. It would help if you generalize this function for future tasks. Feel free to modify input args as needed.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Hc8w76iUmnqp"
   },
   "outputs": [],
   "source": [
    "def visualizing_point_clouds(point_cloud,rgb):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.scatter(point_cloud[:, 0], point_cloud[:, 1], point_cloud[:, 2], color='tab:blue', alpha=1)\n",
    "    ax.set_xlabel('X Axis')\n",
    "    ax.set_ylabel('Y Axis')\n",
    "    ax.set_zlabel('Z Axis')\n",
    "    plt.show()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjiC-DiJwa5a"
   },
   "source": [
    "Write a main function to load data and use the functions you defined above to produce the required output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jtugtmhawaJ9"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/generate_point_cloud.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m      5\u001b[0m     rgb_image \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data = np.load(\"data/generate_point_cloud.npz\")\n",
    "    print(data.keys())\n",
    "    \n",
    "    rgb_image = data[\"rgb\"]\n",
    "    depth_image = data[\"depth\"]\n",
    "    mask = data[\"mask\"]\n",
    "    camera_data = {\"focal_length\": data[\"focal_length\"], \"camera_center\": data[\"camera_center\"]}\n",
    "\n",
    "    point_cloud, rgb = generate_point_clouds(rgb_image, depth_image, mask, camera_data)\n",
    "    visualize_point_clouds(point_cloud, rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFiD1CLGnqfs"
   },
   "source": [
    "# Iterative Closest Point Algorithm\n",
    "\n",
    "When working with real world point cloud data, one often encounters 2 sets points that are unaligned but represent the same scene or object. These point clouds might be slightly rotated, translated, or even have non-uniform scaling due to various factors such as sensor noise, calibration errors, or deformations in the object itself. In such cases, accurately aligning these point clouds becomes essential to make meaningful comparisons or perform further analysis. The Iterative Closest Point (ICP) algorithm is a powerful tool that addresses this challenge, providing a systematic and efficient way to find the optimal transformation that aligns two point clouds or shapes.\n",
    "\n",
    "In this section, we are going to develop an in-depth understanding of the ICP algorithm by implementing it and computing a rigid transformation between two point clouds.\n",
    "\n",
    "Pseudo Code:\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1XfswHyZWBLb06chYDQRc6Ze6Oiwkxmpd)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Q2a. Estimate Correspondences between the two point clouds.\n",
    "You are given two point clouds X and Y, an initial guess of transformation T and rotation R and a threshold for maximum distance between two points.\n",
    "Return a list of estimated point correspondences. Procedure to follow is given in the pseudocode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0G-kSr_Ylkdu"
   },
   "outputs": [],
   "source": [
    "def estimate_correspondences(X, Y, t, R, threshold):\n",
    "    \"\"\"\n",
    "    Estimate Correspondences between two point clouds.\n",
    "\n",
    "    This function takes two point clouds, X and Y, along with an initial guess of\n",
    "    translation 't' and rotation 'R', and a threshold value for the maximum distance\n",
    "    between two points to consider them as correspondences.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy.ndarray): The first point cloud represented as an N x 3 numpy array,\n",
    "                       where N is the number of points.\n",
    "    Y (numpy.ndarray): The second point cloud represented as an M x 3 numpy array,\n",
    "                       where M is the number of points.\n",
    "    t (numpy.ndarray): The initial guess for translation, a 1 x 3 numpy array.\n",
    "    R (numpy.ndarray): The initial guess for rotation, a 3 x 3 numpy array.\n",
    "    threshold (float): The maximum distance between two points to consider them as\n",
    "                       correspondences.\n",
    "\n",
    "    Returns:\n",
    "    correspondences (numpy.ndarray): A numpy array of estimated point correspondences, where each\n",
    "                            correspondence is [x, y], where 'x' is the index of point\n",
    "                            from point cloud X, and 'y' is is the index of a point from point cloud Y.\n",
    "    \"\"\"\n",
    "    correspondences = None\n",
    "    #########################################\n",
    "    #############YOUR CODE HERE##############\n",
    "    #########################################\n",
    "\n",
    "\n",
    "    ##########################################\n",
    "    return correspondences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sB6GiF48j88J"
   },
   "source": [
    "## Q2b. Estimating Optimal Rigid Transform\n",
    "\n",
    "Now that we have computed the correspondences, it is time to compute the transform between them. The algorithm to accomplish this task is mentioned below:\n",
    "![](https://drive.google.com/uc?export=view&id=14JANZvk4sO4CWNqQNhKJx6PT7l7hpKM3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nh2bGSeUj8PU"
   },
   "outputs": [],
   "source": [
    "def compute_rigid_transformation(X,Y,correspondences):\n",
    "    \"\"\"\n",
    "    Estimate the optimal rigid transformation between two point clouds.\n",
    "\n",
    "    Given two point clouds X and Y, along with a list of estimated point correspondences,\n",
    "    this function calculates the optimal rotation and translation that best aligns\n",
    "    point cloud X with point cloud Y.\n",
    "\n",
    "    Parameters:\n",
    "    X (numpy.ndarray): The first point cloud represented as an N x 3 numpy array,\n",
    "                       where N is the number of points.\n",
    "    Y (numpy.ndarray): The second point cloud represented as an M x 3 numpy array,\n",
    "                       where M is the number of points.\n",
    "    correspondences (numpy.ndarray): A numpy array of estimated point correspondences, where each\n",
    "                            correspondence is [x, y], where 'x' is the index of point\n",
    "                            from point cloud X, and 'y' is is the index of a point from point cloud Y.\n",
    "\n",
    "    Returns:\n",
    "    rotation (numpy.ndarray): The estimated rotation matrix (3x3) that best aligns\n",
    "                             point cloud X with point cloud Y.\n",
    "    translation (numpy.ndarray): The estimated translation vector (1x3) that best\n",
    "                                aligns point cloud X with point cloud Y.\n",
    "    \"\"\"\n",
    "    rotation,transformation = None\n",
    "    #########################################\n",
    "    #############YOUR CODE HERE##############\n",
    "    #########################################\n",
    "\n",
    "\n",
    "    ##########################################\n",
    "    return rotation, transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWo0uG5Fk-GA"
   },
   "source": [
    "## Q2c. Stitching Everything Together\n",
    "\n",
    "i. Use the functions you defined in the previous questions and implement the ICP algorithm.\n",
    "\n",
    "ii. Test the algorithm by using the point clouds in the files ```data/point_cloud_X.txt``` and ```data/point_cloud_Y.txt```. Set initial estimate of ```t``` as 0, ```R``` as identity, ```dmax``` as 0.25 and run the code for 30 iterations\n",
    "\n",
    "iii. Report the RMSE error computed from the following equation:\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1cVLvZwm9T1Rj9CKHzQYdZDaokoWqup0O)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TqEBYOkEk9y2"
   },
   "outputs": [],
   "source": [
    "def ICP_algorithm(X , Y , t, R, threshold, max_iter):\n",
    "    #########################################\n",
    "    #############YOUR CODE HERE##############\n",
    "    #########################################\n",
    "    correspondences = None\n",
    "\n",
    "    ##########################################\n",
    "    return t,R,correspondences\n",
    "\n",
    "\n",
    "def RMSE_error(X, Y,t,R):\n",
    "    RMSE = None\n",
    "\n",
    "    return RMSE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#---- Write your 'main' function to load data and use the ICP algorithm -------#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDEY30spv_WE"
   },
   "source": [
    "## Q2d. Visualizing the point clouds\n",
    "Write a function to visualize the two point clouds before and after transformation to witness the might of the ICP algorithm. You are free to use the function you wrote previously for Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9adJ457v_AD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnSUrvF1n25r"
   },
   "source": [
    "# Pointnets\n",
    "\n",
    "PointNet is a groundbreaking neural network architecture tailored for the classification and segmentation of point cloud data. Introduced in the seminal paper \"PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation\" by Charles R. Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas, PointNet represents a significant advancement in the field of 3D deep learning.\n",
    "\n",
    "Processing point clouds poses unique challenges compared to traditional 2D image data. Point clouds are unordered, meaning that the order of the points within the cloud does not inherently convey information. Furthermore, point clouds may have varying numbers of points, making it challenging to apply standard convolutional neural networks (CNNs) designed for regular grids.\n",
    "\n",
    "\n",
    "In the next few sections, we are going to implement the classification and segmentation network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4_-iSQEw1RV"
   },
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Lbblsprwepk"
   },
   "outputs": [],
   "source": [
    "class cocoDataset(Dataset):\n",
    "    def __init__(self, path, train=True):\n",
    "        self.path = path\n",
    "        if train:\n",
    "            self.data_path = self.path + \"/train_data.npy\"\n",
    "            self.label_path = self.path + \"/train_labels.npy\"\n",
    "        else:\n",
    "            self.data_path = self.path + \"/test_data.npy\"\n",
    "            self.label_path = self.path + \"/test_labels.npy\"\n",
    "        self.data = torch.from_numpy(np.load(self.data_path))\n",
    "        self.label = torch.from_numpy(np.load(self.label_path)).to(torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size()[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.label[idx]\n",
    "\n",
    "def get_data_loader(path, batch_size, train=True):\n",
    "    \"\"\"\n",
    "    Creates training and test data loaders\n",
    "    \"\"\"\n",
    "    dataset = cocoDataset(path, train)\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=train, num_workers=1)\n",
    "\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7oEeoOK5s5QB"
   },
   "source": [
    "# Q3. Classification\n",
    "\n",
    "In this question, you are given point clouds of objects corresponding to 3 different classes: Vase, Chair and Lamp . You are to implement the pointnet neural network and train it to differentiate between different objects.\n",
    "\n",
    "Note: Use of a GPU is recommended. You can switch to a GPU runtime by going to ```Runtime>Change Runtime Type```\n",
    "\n",
    "\n",
    "## Q3a. Implementation of Pointnet Architecture.\n",
    "\n",
    "Refer to the Diagram below to implement the structure of Pointnet for classification.\n",
    "\n",
    "```Note1: The values in brackets correspond to layers. After every layer there is a 1D Batch normalization layer and a Relu activation layer. Refer to pytorch documentation on how to implement these layers.```\n",
    "\n",
    "```Note2: Linear layers can also be implemented as 1D Convolutions with Kernel size 1```\n",
    "\n",
    "```Note3: Remember to use a softmax layer to get the output scores```\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=19k9E94pHVrv8QyNoedNIUi2D-tnDi95t)\n",
    "\n",
    "Source: [PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation](https://doi.org/10.48550/arXiv.1612.00593)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IJTkuRxdu447"
   },
   "outputs": [],
   "source": [
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, num_classes = 3, num_points = 1000):\n",
    "        super(ClassificationModel,self).__init__()\n",
    "        #########################################\n",
    "        #############YOUR CODE HERE##############\n",
    "        #########################################\n",
    "        pass\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "\n",
    "    def forward(self,points):\n",
    "        output = None\n",
    "        #########################################\n",
    "        #############YOUR CODE HERE##############\n",
    "        #########################################\n",
    "\n",
    "        ##########################################\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5CRW2DLxRqq"
   },
   "source": [
    "## Q3b. Training the network\n",
    "\n",
    "To train the network the following steps need to be followed:\n",
    "\n",
    "\n",
    "i. Check and convert the dimensions into proper order. Recall, the model needs the data to be in ```B x C x N``` format where B is the batch, C is the channels and N is the number of points\n",
    "\n",
    "ii. The dataset contains 10000 points per point clouds. To ease the computation, we are only using 1000 points per point cloud. Slice the data so that you are only using 1000 points.\n",
    "\n",
    "iii. Compute Forward pass\n",
    "\n",
    "iv. Calculate the Loss. We are using the CrossEntropyLoss for classification.\n",
    "\n",
    "v. Perform back propogation and optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m8lllSbgsKYt"
   },
   "outputs": [],
   "source": [
    "def train(train_dataloader, model, opt, epoch, device, task, num_seg_classes = 6):\n",
    "    model.train()\n",
    "    step = epoch*len(train_dataloader)\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        point_clouds, labels = batch\n",
    "        point_clouds = point_clouds.to(device)\n",
    "        labels = labels.to(device).to(torch.long)\n",
    "        #########################################\n",
    "        #############YOUR CODE HERE##############\n",
    "        #########################################\n",
    "\n",
    "        # Compute Forward pass\n",
    "\n",
    "        predictions = None\n",
    "\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "\n",
    "\n",
    "        #########################################\n",
    "        #############YOUR CODE HERE##############\n",
    "        #########################################\n",
    "\n",
    "        # Initialize & Calculate Loss and perform back propogation\n",
    "\n",
    "        loss = None\n",
    "\n",
    "\n",
    "        ##########################################\n",
    "        epoch_loss += loss\n",
    "\n",
    "    return epoch_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kceHIVKv027w"
   },
   "source": [
    "## Q3c. Testing the Network\n",
    "\n",
    "i. Perform the same steps as Q3b to prepare the data for the network.\n",
    "\n",
    "ii. Get the predictions from your network. Remember to not compute gradients or the loss!\n",
    "\n",
    "iii. Calculate the accuracy of the model by comparing how many predictions correctly match the labels and dividing the number by total number of datapoints\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mRR6btHQsJpN"
   },
   "outputs": [],
   "source": [
    "def test(test_dataloader, model, epoch, device):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Evaluation in Classification Task\n",
    "    correct_obj = 0\n",
    "    num_obj = 0\n",
    "    for batch in test_dataloader:\n",
    "        point_clouds, labels = batch\n",
    "        point_clouds = point_clouds.to(device)\n",
    "        labels = labels.to(device).to(torch.long)\n",
    "        #########################################\n",
    "        #############YOUR CODE HERE##############\n",
    "        #########################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #########################################\n",
    "    # Compute Accuracy of Test Dataset\n",
    "    accuracy = correct_obj / num_obj\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IN6fwcFYzUEk"
   },
   "source": [
    "## Running the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ziX5oZuz-R5"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 3\n",
    "learning_rate = 0.001\n",
    "path = \"data/cls\"\n",
    "batch_size = 32\n",
    "num_epochs = 150\n",
    "model_save_path = os.path.join(os. getcwd(), 'model/best_model.pt')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "model = classification_model(NUM_CLASSES).to(device)\n",
    "opt = optim.Adam(model.parameters(), learning_rate, betas=(0.9, 0.999))\n",
    "train_dataloader = get_data_loader(path,batch_size, True)\n",
    "test_dataloader = get_data_loader(path,batch_size, False)\n",
    "print(\"++++++ SUCCESSFULLY LOADED DATA ++++++\")\n",
    "\n",
    "best_accuracy = -1\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    train_epoch_loss = train(train_dataloader, model, opt, epoch, device, \"cls\")\n",
    "\n",
    "    test_accuracy = test(test_dataloader, model, epoch, device, \"cls\")\n",
    "    print (\"epoch: {}   train loss: {:.4f}   test accuracy: {:.4f}\".format(epoch, train_epoch_loss, test_accuracy))\n",
    "\n",
    "    # if (test_accuracy > best_accuracy):\n",
    "    #     best_accuracy = test_accuracy\n",
    "        # torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "print(\"===============TRAINING COMPLETE===============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yF37sDWL1sDR"
   },
   "source": [
    "## Q3d. Inference\n",
    "\n",
    "i. Visualize few point clouds that have been classified correctly\n",
    "\n",
    "ii. Visualize few point clouds that have been classfied incorrectly\n",
    "\n",
    "You can re-use functions defined previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h8yUun6D1q7v"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdKzZ20Df1Wg"
   },
   "source": [
    "# Q4. Segmentation\n",
    "\n",
    "In this question, you are given a point cloud of different types of chairs with labels or classes for different parts of the chair: back rest, arm rest, seat etc. Your task is to create a PointNet segmentation network and train it to perform part segmentation.\n",
    "\n",
    "Note: Use of a GPU is recommended. You can switch to a GPU runtime by going to ```Runtime>Change Runtime Type```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFV-XbimjVVz"
   },
   "source": [
    "## Q4a. Implementation of the Pointnet architecture for segmentation\n",
    "\n",
    "The architecture of pointnet for segmentation is given below:\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1gATiXB-CD6LmetMjylBbg230CoowyVud)\n",
    "Source: [PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation](https://doi.org/10.48550/arXiv.1612.00593)\n",
    "\n",
    "There are a total of 6 classes that the points can be segmented into.\n",
    "\n",
    "For segmentation, we store the local features extracted after the first set of MLP layers and concatenate each point in the local feature tensor with the global feature vector, making the input to the segmentation network ```n x (64+ 1024) =  nx1088```\n",
    "\n",
    "```Note1: The values in brackets correspond to layers. After every layer there is a 1D Batch normalization layer and a Relu activation layer. Refer to pytorch documentation on how to implement these layers.```\n",
    "\n",
    "```Note2: Your implementation should not contain the input transform and the feature transform```\n",
    "\n",
    "```Note3: MLP can also be implemented as 1D Convolutions with Kernel size 1```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-KRyQWARjU9L"
   },
   "outputs": [],
   "source": [
    "class SegmentationNetwork(nn.Module):\n",
    "    def __init__(self, num_points = 2000, num_seg_classes = 6):\n",
    "        super(SegmentationNetwork,self).__init__()\n",
    "        #########################################\n",
    "        #############YOUR CODE HERE##############\n",
    "        #########################################\n",
    "        pass\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "\n",
    "    def forward(self,points):\n",
    "        \"\"\"\n",
    "        Implement the forward pass\n",
    "        Parameters:\n",
    "        points (torch.Tensor): Tensor of shape (BxCxN) containing point cloud data\n",
    "        where B is the batch size, C is number of channels, N is number of points in the point cloud\n",
    "\n",
    "        Returns:\n",
    "        output (torch.Tensor): Tensor of shape (B x num_seg_classes x N)\n",
    "        \"\"\"\n",
    "        output = None\n",
    "        #########################################\n",
    "        #############YOUR CODE HERE##############\n",
    "        #########################################\n",
    "\n",
    "        ##########################################\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Iu4Y7kUlk4z"
   },
   "source": [
    "## Q4b. Implementing the loss function\n",
    "\n",
    "In this case, we need a special loss function because our data is not uniformly distributed. To illustrate it further run the following block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "id": "isg_8cKtlkd4",
    "outputId": "8d5110cc-3c14-4391-fd04-ab096a8a8ac2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## RUN this block for the plot\n",
    "CATEGORIES = {'1': 0, '2': 1, '3': 2, '4': 3, '5': 4, '6': 5}\n",
    "COLOR_MAP = {\n",
    "    0  : (47, 79, 79),\n",
    "    1  : (139, 69, 19),\n",
    "    2  : (34, 139, 34),\n",
    "    3  : (75, 0, 130),\n",
    "    4  : (255, 0, 0),\n",
    "    5  : (255, 255, 0)}\n",
    "v_map_colors = np.vectorize(lambda x : COLOR_MAP[x])\n",
    "NUM_CLASSES = len(CATEGORIES)\n",
    "total_train_targets = []\n",
    "train_dataloader = get_data_loader(\"data/seg\",32, True)\n",
    "for (_, targets) in train_dataloader:\n",
    "    total_train_targets += targets.reshape(-1).numpy().tolist()\n",
    "\n",
    "total_train_targets = np.array(total_train_targets)\n",
    "class_bins = np.bincount(total_train_targets)\n",
    "\n",
    "plt.bar(list(CATEGORIES.keys()), class_bins,\n",
    "             color=[np.array(val)/255. for val in list(COLOR_MAP.values())],\n",
    "             edgecolor='black')\n",
    "plt.xticks(list(CATEGORIES.keys()), list(CATEGORIES.keys()), size=12, rotation=90)\n",
    "plt.ylabel('Counts', size=12)\n",
    "plt.title('Frequency of Each Category', size=14, pad=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PU50B40Hq7ap"
   },
   "source": [
    "We observe a significant imbalance in the dataset. Penalizing the network equally for misclassifying categories of varying frequencies wouldn't be logical. If we penalize high-frequency categories the same as low-frequency ones, the model might excel at capturing common patterns but struggle with rare features.\n",
    "\n",
    "For this reason, we define a few weights for the loss function that correspond to the different classes. These are learnable parameters and we are initializing them manually as per the visualization. You are free to experiment with them as you please."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VaUHuzkmq66v"
   },
   "outputs": [],
   "source": [
    "alpha = np.ones(6)\n",
    "alpha[5] *= 0.25\n",
    "alpha[2] *= 0.35\n",
    "alpha[4] *= 0.45\n",
    "alpha[3] *= 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWzvFL-btXKS"
   },
   "source": [
    "Initialize the cross entropy loss with the weights and perform the forward pass. Refer to the pytorch documentation on Cross Entropy loss for  more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LDk-XYKdq6Gy"
   },
   "outputs": [],
   "source": [
    "class PointNetSegLoss(nn.Module):\n",
    "    def __init__(self, alpha):\n",
    "        super(PointNetSegLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        #########################################\n",
    "        #############YOUR CODE HERE##############\n",
    "        #########################################\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        loss = None\n",
    "        #########################################\n",
    "        #############YOUR CODE HERE##############\n",
    "        #########################################\n",
    "\n",
    "        ##########################################\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwoP1fGdlZrI"
   },
   "source": [
    "## Q4c. Training the Network\n",
    "\n",
    "To train the network the following steps need to be followed:\n",
    "\n",
    "i. Check and convert the dimensions into proper order. Recall, the model needs the data to be in ```B x C x N``` format where B is the batch, C is the channels and N is the number of points\n",
    "\n",
    "ii. The dataset contains 10000 points per point clouds. To reduce computation, we are only using 2000 points per point cloud. Slice the data so that you are only using 2000 points.\n",
    "\n",
    "iii. Compute Forward pass\n",
    "\n",
    "iv. Calculate the Loss. Here the ```PointNetSegLoss``` is passed as the argument ```criterion``` in the function\n",
    "\n",
    "v. Perform back propogation and optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0_2MO92hf05_"
   },
   "outputs": [],
   "source": [
    "def train_segmentation_model(train_dataloader, model, opt, epoch, device, criterion, num_seg_classes = 6):\n",
    "    model.train()\n",
    "\n",
    "    step = epoch*len(train_dataloader)\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        point_clouds, labels = batch\n",
    "        point_clouds = point_clouds.to(device)\n",
    "        labels = labels.to(device).to(torch.long)\n",
    "        #########################################\n",
    "        #############YOUR CODE HERE##############\n",
    "        #########################################\n",
    "\n",
    "        # Compute Forward pass\n",
    "\n",
    "        predictions = None\n",
    "\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #########################################\n",
    "        #############YOUR CODE HERE##############\n",
    "        #########################################\n",
    "\n",
    "        # Initialize & Calculate Loss and perform back propogation\n",
    "\n",
    "        loss = None\n",
    "\n",
    "\n",
    "        ##########################################\n",
    "        epoch_loss += loss\n",
    "\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1FZp6_exHsL"
   },
   "source": [
    "## Q4d. Testing the Network\n",
    "\n",
    "i. Perform the same steps as Q4c to prepare the data for the network.\n",
    "\n",
    "ii. Get the predictions from your network. Remember to not compute gradients or the loss!\n",
    "\n",
    "iii. Get the prediction with highest probability from the output by using ```torch.softmax```\n",
    "\n",
    "iv. Calculate the accuracy of the model by comparing how many predictions correctly match the labels and dividing the number by total number of datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_nL-Gk-xX33"
   },
   "outputs": [],
   "source": [
    "def test_segmentation_model(test_dataloader, model, epoch, device):\n",
    "\n",
    "    model.eval()\n",
    "    correct_point = 0\n",
    "    num_point = 0\n",
    "    for batch in test_dataloader:\n",
    "        point_clouds, labels = batch\n",
    "        point_clouds = point_clouds.to(device)\n",
    "        labels = labels.to(device).to(torch.long)\n",
    "        #########################################\n",
    "        #############YOUR CODE HERE##############\n",
    "        #########################################\n",
    "\n",
    "        # Compute Forward pass and evaluate the model\n",
    "\n",
    "        predictions = None\n",
    "\n",
    "\n",
    "        ##########################################\n",
    "    # Compute Accuracy of Test Dataset\n",
    "    accuracy = correct_point / num_point\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_8lFARNzYZr"
   },
   "source": [
    "## Running the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ux0SvjWGzYLc"
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 6\n",
    "NUM_TRAIN_POINTS = 2000\n",
    "learning_rate = 0.001\n",
    "path = \"data/seg\"\n",
    "batch_size = 32\n",
    "num_epochs = 200\n",
    "model = SegmentationNetwork(NUM_TRAIN_POINTS, NUM_CLASSES)\n",
    "model_save_path = os.path.join(os. getcwd(), 'model/best_seg_model.pt')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "opt = optim.Adam(model.parameters(), learning_rate)\n",
    "train_dataloader = get_data_loader(path,batch_size, True)\n",
    "test_dataloader = get_data_loader(path,batch_size, False)\n",
    "\n",
    "\n",
    "alpha = np.ones(6)\n",
    "alpha[-1] *= 0.25\n",
    "alpha[2] *= 0.35\n",
    "alpha[4] *= 0.45\n",
    "alpha[3] *= 0.5\n",
    "\n",
    "criterion = PointNetSegLoss(alpha=alpha).to(device)\n",
    "\n",
    "model = model.to(device)\n",
    "print(\"++++++ SUCCESSFULLY LOADED DATA ++++++\")\n",
    "\n",
    "best_accuracy = -1\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    train_epoch_loss = train_segmentation_model(train_dataloader, model, opt, epoch, device, criterion)\n",
    "\n",
    "    test_accuracy = test_segmentation_model(test_dataloader, model, epoch, device)\n",
    "    print (\"epoch: {}   train loss: {:.4f}   test accuracy: {:.4f}\".format(epoch, train_epoch_loss, test_accuracy))\n",
    "\n",
    "    if (test_accuracy > best_accuracy):\n",
    "        best_accuracy = test_accuracy\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "print(\"===============TRAINING COMPLETE===============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NNnE5C9z0tCT"
   },
   "source": [
    "## Q4e. Inference\n",
    "\n",
    "i. Visualize a few point clouds that have been segmented correctly. Make sure to use different colours for different classes\n",
    "\n",
    "ii. Visualize few point clouds that have been segmented incorrectly. Make sure to use different colours for different classes\n",
    "\n",
    "You can re-use functions defined previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LV29c2Fi0sl1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkR5z6h5Xktx"
   },
   "source": [
    "# Submission Instructions\n",
    "Run the entire notebook and make sure all outputs are visible. ```We will NOT run your code.```\n",
    "\n",
    "Rename the notebook as ```uniquename_assignment2.ipynb``` and submit the link of the notebook on canvas. Make sure to set proper access for your link so that anyone with the link can ```view``` your notebook\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
